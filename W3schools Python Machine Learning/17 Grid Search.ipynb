{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "The majority of machine learning models contain parameters that can be adjusted to vary how the model learns. For example, the logistic regression model, from sklearn, has a parameter C that controls regularization,which affects the complexity of the model.\n",
    "\n",
    "How do we pick the best value for C? The best value is dependent on the data used to train the model.\n",
    "\n",
    "How does it work?\n",
    "One method is to try out different values and then pick the value that gives the best score. This technique is known as a grid search. If we had to select the values for two or more parameters, we would evaluate all combinations of the sets of values thus forming a grid of values.\n",
    "\n",
    "Higher values of C tell the model, the training data resembles real world information, place a greater weight on the training data. While lower values of C do the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9666666666666667, 0.9666666666666667, 0.9733333333333334, 0.9733333333333334, 0.98, 0.98, 0.9866666666666667, 0.9866666666666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "logit = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "C = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]\n",
    "\n",
    "scores = []\n",
    "\n",
    "for choice in C:\n",
    "  logit.set_params(C=choice)\n",
    "  logit.fit(X, y)\n",
    "  scores.append(logit.score(X, y))\n",
    "\n",
    "print(scores)\n",
    "# We can see that the lower values of C performed worse than the base parameter of 1. \n",
    "# As we increased the value of C to 1.75 the model experienced increased accuracy.\n",
    "# It seems that increasing C beyond this amount does not help increase model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scored our logistic regression model by using the same data that was used to train it. If the model corresponds too closely to that data, it may not be great at predicting unseen data. This statistical error is known as over fitting.\n",
    "\n",
    "To avoid being misled by the scores on the training data, we can put aside a portion of our data and use it specifically for the purpose of testing the model. Refer to the lecture on train/test splitting to avoid being misled and overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
